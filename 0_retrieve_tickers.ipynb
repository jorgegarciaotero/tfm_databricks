{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "379ff251-2a54-4a29-9d35-3a7e74efb0eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Populate the list of tickers whose information will be fetched later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e1907b95-947f-4a4b-bb02-86ea58059b19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install lxml\n",
    "%pip install html5lib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0f2e3a6-772c-4d9c-82b4-b690b605650c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1afaf00b-82bb-4c71-8896-d4a250c3d6ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/jorgegarciaotero@gmail.com/config/database_connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afdc9e0d-fd43-4cd4-b9b2-7e2b85187cbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/jorgegarciaotero@gmail.com/config/logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "25715417-3266-42ac-88f7-03825a42a12f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import numpy as np\n",
    "from datetime import datetime, date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "84d4a075-217c-47c6-b82b-7f6b5829d1f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_tickers_from_csv(url, skiprows=0, sep=','):\n",
    "    \"\"\"\n",
    "    Extract tickers from a remote CSV file (like iShares ETF holdings).\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the CSV file to download.\n",
    "        skiprows (int): Number of rows to skip at the top.\n",
    "        sep (str): Delimiter (default ',')\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Parsed DataFrame with holdings.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0',\n",
    "        'Accept': 'text/csv',\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        content = response.text\n",
    "\n",
    "        df = pd.read_csv(StringIO(content), skiprows=skiprows, sep=sep,\n",
    "                         quotechar='\"', thousands=\",\", decimal=\".\", on_bad_lines=\"skip\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Error fetching CSV from {url}: {e}\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c21b5246-1162-489d-b219-b72c57050691",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_ftse100_tickers_wiki():\n",
    "    \"\"\"\n",
    "    Extract FTSE 100 tickers from Wikipedia using BeautifulSoup + pandas (Databricks-friendly).\n",
    "    \n",
    "    Returns:\n",
    "        list: List of ticker symbols with .L suffix.\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/wiki/FTSE_100_Index\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        tables = soup.find_all(\"table\", {\"class\": \"wikitable\"})\n",
    "\n",
    "        if not tables:\n",
    "            print(\"‚ùå No tables found on Wikipedia page.\")\n",
    "            return []\n",
    "\n",
    "        # Buscar la tabla que tenga una columna llamada 'Ticker'\n",
    "        for i, table in enumerate(tables):\n",
    "            try:\n",
    "                df = pd.read_html(str(table), flavor=\"bs4\")[0]\n",
    "                if \"Ticker\" in df.columns:\n",
    "                    tickers = df[\"Ticker\"].dropna().unique().tolist()\n",
    "                    tickers = [f\"{t}.L\" for t in tickers]\n",
    "                    return tickers\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        print(\"‚ùå No table with column 'Ticker' found.\")\n",
    "        return []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching FTSE100 tickers: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8e44403-83fb-4882-96ad-a64509debe35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def upsert_data_simple(db, table_name, df, pk_columns, logger):\n",
    "    \"\"\"\n",
    "    UPSERT (insert or update) a DataFrame into SQL Server using a MERGE statement.\n",
    "\n",
    "    Args:\n",
    "        db: Connection object with the method execute_jdbc_query(query)\n",
    "        table_name (str): Target table name\n",
    "        df (pd.DataFrame): DataFrame with the data to insert or update\n",
    "        pk_columns (list): List of primary key column names\n",
    "        logger: Logger used to track errors and status messages\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"üîÅ Upserting {len(df)} rows into {table_name}\")\n",
    "        df = df.replace({np.nan: None, np.inf: None, -np.inf: None})\n",
    "\n",
    "        for row in df.itertuples(index=False, name=None):\n",
    "            values = []\n",
    "            for val in row:\n",
    "                if val is None or (isinstance(val, float) and np.isnan(val)):\n",
    "                    values.append(\"NULL\")\n",
    "                elif isinstance(val, str):\n",
    "                    val = val.replace(\"'\", \"''\")\n",
    "                    values.append(f\"'{val}'\")\n",
    "                elif isinstance(val, (pd.Timestamp, datetime, date, np.datetime64)):\n",
    "                    values.append(f\"'{str(val)}'\")\n",
    "                else:\n",
    "                    values.append(f\"'{val}'\")\n",
    "\n",
    "            columns = df.columns.tolist()\n",
    "            on_clause = \" AND \".join(f\"target.[{col}] = source.[{col}]\" for col in pk_columns)\n",
    "            update_clause = \", \".join(f\"target.[{col}] = source.[{col}]\" for col in columns if col not in pk_columns)\n",
    "            insert_columns = \", \".join(f\"[{col}]\" for col in columns)\n",
    "            insert_values = \", \".join(f\"source.[{col}]\" for col in columns)\n",
    "            source_values = \", \".join(f\"{val} AS [{col}]\" for val, col in zip(values, columns))\n",
    "\n",
    "            query = f\"\"\"\n",
    "                MERGE INTO {table_name} AS target\n",
    "                USING (SELECT {source_values}) AS source\n",
    "                ON {on_clause}\n",
    "                WHEN MATCHED THEN \n",
    "                    UPDATE SET {update_clause}\n",
    "                WHEN NOT MATCHED THEN \n",
    "                    INSERT ({insert_columns}) VALUES ({insert_values});\n",
    "            \"\"\"\n",
    "\n",
    "            try:\n",
    "                db.execute_jdbc_query(query)\n",
    "            except Exception as row_err:\n",
    "                logger.error(f\"‚ùå Error upserting row {row}: {row_err}\")\n",
    "                logger.debug(f\"Query: {query}\")\n",
    "\n",
    "        logger.info(\"‚úÖ Upsert finished.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Upsert failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f00f18b-a969-44f6-aac7-51c13c598da0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# URLs de iShares\n",
    "url_russell2000 = \"https://www.ishares.com/us/products/239710/ishares-russell-2000-etf/1467271812596.ajax?fileType=csv&fileName=IWM_holdings&dataType=fund\"\n",
    "url_sp500 = \"https://www.ishares.com/us/products/239726/ishares-core-sp-500-etf/1467271812596.ajax?fileType=csv&fileName=IVV_holdings&dataType=fund\"\n",
    "\n",
    "df_russell = get_tickers_from_csv(url_russell2000, skiprows=9)\n",
    "df_sp500 = get_tickers_from_csv(url_sp500, skiprows=9)\n",
    "ftse100_tickers=get_ftse100_tickers_wiki()\n",
    "df_russell = df_russell.iloc[:-2]\n",
    "df_sp500 = df_sp500.iloc[:-2]\n",
    "\n",
    "df_sp500['source']='sp500'\n",
    "df_russell['source']='rusell200'\n",
    "df_ftse100=pd.DataFrame(ftse100_tickers,columns=['Ticker'])\n",
    "df_ftse100['source']='ftse100'\n",
    "df_all=pd.concat([df_sp500,df_russell,df_ftse100],ignore_index=True)\n",
    "df_all['ingest_date']=pd.to_datetime('today')\n",
    "\n",
    "df_all.rename(columns={'Ticker':'symbol'},inplace=True)\n",
    "df_all=df_all[['symbol','source','ingest_date']]\n",
    "\n",
    "df_all=df_all[df_all['symbol']!='-']\n",
    "df_all=df_all.drop_duplicates('symbol')\n",
    "print(f\"shape: {df_all.shape}\")\n",
    "\n",
    "db =  DatabaseConnector()\n",
    "for col in df_all.columns:\n",
    "    df_all[col] = df_all[col].apply(lambda x: str(x) if isinstance(x, (dict, list, tuple)) else x)\n",
    "logger = get_logger(name=\"my_app\", level=\"INFO\", log_file=\"retrieve_tickets.log\")\n",
    "\n",
    "upsert_data_simple(\n",
    "    db=db,\n",
    "    table_name=\"tickers\",\n",
    "    df=df_all,\n",
    "    pk_columns=[\"symbol\"],  # ‚Üê tu clave primaria\n",
    "    logger=logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f3493a0-f4dd-4a70-87de-e7f51b86a53d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "0_retrieve_tickers",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
