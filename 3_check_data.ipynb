{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "589aa0a4-db52-4075-a8c0-6334f355e607",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/jorgegarciaotero@gmail.com/tfm_databricks/config/database_connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "689a5a8b-71b8-41e2-bc3d-69e235daa9c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8fab2a9-b822-4e22-abaf-abba055dcef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### FUNCIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d66b036-1672-4315-8556-a3aaaa7a67fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_null_values(df) -> None:\n",
    "    \"\"\"\n",
    "    Checks for null values in the DataFrame and displays the count of null values for each column.\n",
    "\n",
    "    ARGS:\n",
    "        df: Spark DataFrame\n",
    "\n",
    "    RETURNS:\n",
    "        None\n",
    "    \"\"\"\n",
    "    excluded_cols = ['date', 'symbol']\n",
    "    cols_to_check = [c for c in df.columns if c not in excluded_cols]\n",
    "\n",
    "    # Create list of (column_name, count expression)\n",
    "    null_exprs = [\n",
    "        (c, F.count(F.when(F.col(c).isNull() | F.isnan(c), c)).alias(c))\n",
    "        for c in cols_to_check\n",
    "    ]\n",
    "\n",
    "    # Select row with all null counts\n",
    "    null_row = df.select([expr for _, expr in null_exprs])\n",
    "\n",
    "    # Convert to long format using stack\n",
    "    stacked = null_row.select(F.expr(\"stack({}, {})\".format(\n",
    "        len(null_exprs),\n",
    "        \", \".join([f\"'{c}', {c}\" for c, _ in null_exprs])\n",
    "    )).alias(\"column\", \"nulls\"))\n",
    "\n",
    "    # Filter and sort\n",
    "    result = stacked.filter(\"nulls > 0\").orderBy(F.desc(\"nulls\"))\n",
    "\n",
    "    display(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f242b3fe-ba7b-4477-9a6b-0a2807f31358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def remove_initial_days_per_symbol(df, min_days=20):\n",
    "    \"\"\"\n",
    "    Removes the first `min_days` rows per symbol based on date order.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Spark DataFrame with at least ['symbol', 'date']\n",
    "        min_days (int): Number of initial rows to drop per symbol\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Cleaned DataFrame with initial rows removed\n",
    "    \"\"\"\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql.functions import row_number\n",
    "\n",
    "    w = Window.partitionBy(\"symbol\").orderBy(\"date\")\n",
    "    df = df.withColumn(\"row_num\", row_number().over(w))\n",
    "    df = df.filter(F.col(\"row_num\") > min_days).drop(\"row_num\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7912bb0-a992-402e-92a7-20939952327c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d913fe4-7c5b-43a3-a035-e9694c08b16c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creates the input widgets and sets the default values\n",
    "dbutils.widgets.text(\"storage_account\", \"smartwalletjorge\", \"Storage Account\")\n",
    "dbutils.widgets.text(\"container\", \"smart-wallet-dl\", \"Container\")\n",
    "dbutils.widgets.text(\"database\", \"smart_wallet\", \"Database\")\n",
    "\n",
    "storage_account = dbutils.widgets.get(\"storage_account\")\n",
    "container = dbutils.widgets.get(\"container\")\n",
    "database_name = dbutils.widgets.get(\"database\")\n",
    "date_value = dbutils.widgets.get(\"date\")\n",
    "if (date_value is None) or (date_value==''):\n",
    "    date_value=None\n",
    "\n",
    "db_connector = DatabaseConnector()\n",
    "print(f\"database_name :{database_name}\")\n",
    "\n",
    "df=db_connector.read_table_from_path(container, database_name, \"stock_data_parquet\", date_value,\"parquet\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2866c36-9996-436b-b9f7-b4de72e98944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 1. Esquema y nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9c7133b-ea22-46ca-b6f4-ca625b68bb77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Counts : {df.count()}\")\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "851b6e5a-c267-4c20-9ad2-0b91c92d5123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the null values of the dataframe\n",
    "check_null_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1233c9dd-17e8-4ea8-a30f-c83a326d0374",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Target counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ff27786-a2b8-432d-a70a-c03b75de68b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"target_3m\",\n",
    "    when(col(\"ret_next_3m\") > 0.1, 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"target_6m\",\n",
    "    when(col(\"ret_next_6m\") > 0.1, 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"target_1y\",\n",
    "    when(col(\"ret_next_1y\") > 0.1, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"=== target_3m ===\")\n",
    "df.groupBy(\"target_3m\") \\\n",
    "  .count() \\\n",
    "  .orderBy(\"target_3m\") \\\n",
    "  .show()\n",
    "\n",
    "print(\"=== target_6m ===\")\n",
    "df.groupBy(\"target_6m\") \\\n",
    "  .count() \\\n",
    "  .orderBy(\"target_6m\") \\\n",
    "  .show()\n",
    "\n",
    "print(\"=== target_1y ===\")\n",
    "df.groupBy(\"target_1y\") \\\n",
    "  .count() \\\n",
    "  .orderBy(\"target_1y\") \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "180f1d2b-0dc4-434a-9bc8-f4d163870f6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from typing import List\n",
    "\n",
    "def verify_future_return_3m(\n",
    "    df: DataFrame,\n",
    "    symbol: str,\n",
    "    reference_dates: List[str]\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Given a Spark DataFrame that already contains columns:\n",
    "      - price_lead_3m (Double): the stored “lead-close” at ~63 trading days ahead\n",
    "      - ret_next_3m  (Double): the stored return (price_lead_3m - close_v) / close_v\n",
    "\n",
    "    This function computes a manual “close_3m_manual” using lead(close_v, 63)\n",
    "    over a partitionBy(symbol).orderBy(date) window, and then compares:\n",
    "      * price_lead_3m  (precomputed)\n",
    "      * close_3m_manual (lead of close_v 63 rows ahead)\n",
    "      * ret_next_3m     (precomputed)\n",
    "      * ret_3m_manual   (computed as (close_3m_manual - close_v) / close_v)\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "    df : DataFrame\n",
    "        A Spark DataFrame that contains at least:\n",
    "          - symbol (string)\n",
    "          - date (date or string)\n",
    "          - close_v (Double)\n",
    "          - price_lead_3m (Double)\n",
    "          - ret_next_3m  (Double)\n",
    "\n",
    "    symbol : str\n",
    "        The ticker to filter for (e.g. \"AAPL\").\n",
    "\n",
    "    reference_dates : List[str]\n",
    "        A list of date strings (e.g., [\"2023-01-03\", \"2023-02-01\"]) for which\n",
    "        you want to inspect and compare the stored vs. manual values.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    DataFrame\n",
    "        A Spark DataFrame with one row per reference_date, containing:\n",
    "          - date\n",
    "          - close_v\n",
    "          - price_lead_3m       (precomputed)\n",
    "          - close_3m_manual     (computed via lead)\n",
    "          - ret_3m_manual       (computed on the fly)\n",
    "          - ret_next_3m         (precomputed)\n",
    "    \"\"\"\n",
    "    # 1) Define the window partitioned by symbol and ordered by date\n",
    "    window_spec = Window.partitionBy(\"symbol\").orderBy(\"date\")\n",
    "\n",
    "    # 2) Filter for the desired symbol and add a manual \"lead(close_v, 63)\" column\n",
    "    df_symbol = (\n",
    "        df\n",
    "          .filter(F.col(\"symbol\") == symbol)\n",
    "          .withColumn(\"close_3m_manual\", F.lead(\"close_v\", 63).over(window_spec))\n",
    "    )\n",
    "\n",
    "    # 3) Restrict to the reference_dates and select/compute all necessary fields\n",
    "    result = (\n",
    "        df_symbol\n",
    "          .filter(F.col(\"date\").isin(reference_dates))\n",
    "          .select(\n",
    "              F.col(\"date\"),\n",
    "              F.col(\"close_v\"),\n",
    "              F.col(\"price_lead_3m\"),\n",
    "              F.col(\"close_3m_manual\"),\n",
    "              # manual return: (close_3m_manual - close_v) / close_v\n",
    "              ((F.col(\"close_3m_manual\") - F.col(\"close_v\")) / F.col(\"close_v\"))\n",
    "                  .alias(\"ret_3m_manual\"),\n",
    "              F.col(\"ret_next_3m\")\n",
    "          )\n",
    "    )\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "167e158b-8b08-463c-b80e-1b8a46cc3b1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Supón que df_all ya tiene las columnas price_lead_3m y ret_next_3m\n",
    "reference_dates = [\"2023-03-04\", \"2023-04-04\"]\n",
    "df_check = verify_future_return_3m(df, symbol=\"AAPL\", reference_dates=reference_dates)\n",
    "\n",
    "df_check.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00f92ede-22c2-4516-8168-42efac416b90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_apple = df.filter(F.col(\"symbol\") == \"AAPL\")\n",
    "\n",
    "# 2) Seleccionar únicamente las columnas deseadas\n",
    "df_apple_sel = df_apple.select(\n",
    "    \"date\",\n",
    "    \"close_v\",\n",
    "    \"price_lead_3m\",\n",
    "    \"price_lead_6m\",\n",
    "    \"price_lead_1y\"\n",
    ")\n",
    "\n",
    "df_apple_ord = df_apple_sel.orderBy(\"date\")\n",
    "\n",
    "display(df_apple_ord)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3_check_data",
   "widgets": {
    "container": {
     "currentValue": "smart-wallet-dl",
     "nuid": "5ab1907a-4f6a-4820-a1bd-83b776f790f5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "smart-wallet-dl",
      "label": "Container",
      "name": "container",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "smart-wallet-dl",
      "label": "Container",
      "name": "container",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "database": {
     "currentValue": "smart_wallet",
     "nuid": "a6514c90-b072-4fd5-82f0-1ad28733b64a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "smart_wallet",
      "label": "Database",
      "name": "database",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "smart_wallet",
      "label": "Database",
      "name": "database",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "date": {
     "currentValue": "",
     "nuid": "0ac86826-1ce7-4fb3-b16a-4fbf8b68b7b2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2024-04-10",
      "label": "Date",
      "name": "date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2024-04-10",
      "label": "Date",
      "name": "date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "storage_account": {
     "currentValue": "smartwalletjorge",
     "nuid": "9f3375d3-cda2-411d-a05e-74f39c4948b9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "smartwalletjorge",
      "label": "Storage Account",
      "name": "storage_account",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "smartwalletjorge",
      "label": "Storage Account",
      "name": "storage_account",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
