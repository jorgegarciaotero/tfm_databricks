{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecc83cec-3eaa-4b16-93d1-bb90c97cf577",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from datetime import date,datetime\n",
    "import traceback\n",
    "import os\n",
    "import pyodbc\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "class DatabaseConnector:\n",
    "    \"\"\"\n",
    "    A class to handle SQL Server database connections and operations.\n",
    "    \n",
    "    Attributes:\n",
    "        server (str): The server address.\n",
    "        database (str): The database name.\n",
    "        username (str): The username for authentication.\n",
    "        password (str): The password for authentication.\n",
    "        conn (pyodbc.Connection): The connection object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __with_retry(self, func, retries=3, delay=30, retry_message=None):\n",
    "        \"\"\"\n",
    "        Executes a function with retries and delay.\n",
    "\n",
    "        Args:\n",
    "            func (callable): Function to be executed.\n",
    "            retries (int): Number of retries.\n",
    "            delay (int): Delay between retries in seconds.\n",
    "            retry_message (str):Optional message to print on retry failure.\n",
    "\n",
    "        Returns:\n",
    "            Result of the function if successful, otherwise None.\n",
    "        \"\"\"\n",
    "        for attempt in range(1, retries + 1):\n",
    "            try:\n",
    "                return func()\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Retry {attempt}/{retries}: {e}\")\n",
    "                if retry_message:\n",
    "                    print(retry_message)\n",
    "                if attempt < retries:\n",
    "                    time.sleep(delay)\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the Database class with connection settings from a YAML file.\n",
    "        \n",
    "        Args:\n",
    "            config_file (str): Path to the YAML configuration file.\n",
    "        \"\"\"\n",
    "        base_dir = os.getcwd()\n",
    "        config_path = os.path.join(f\"{base_dir}/config\", \"config.yml\")\n",
    "\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        self.server = config['database']['server']\n",
    "        self.database = config['database']['database']\n",
    "        self.username = config['database']['username']\n",
    "        self.password = config['database']['password']\n",
    "        self.port = config['database']['port']\n",
    "        self.client_id = config['database']['client_id']\n",
    "        self.jdbc_url = (\n",
    "            f\"jdbc:sqlserver://{self.server}:{self.port};\"\n",
    "            f\"database={self.database};\"\n",
    "            \"encrypt=true;\"\n",
    "            \"trustServerCertificate=false;\"\n",
    "            \"hostNameInCertificate=*.database.windows.net;\"\n",
    "            \"loginTimeout=30;\"\n",
    "        )\n",
    "        self.connection_properties = {\n",
    "            \"user\": self.username,\n",
    "            \"password\": self.password,\n",
    "            \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "        }\n",
    "            \n",
    "    def read_table_from_sql(self, table_name, date_value=None):\n",
    "        \"\"\"\n",
    "         Reads a table from the database, filtering by the specified date,\n",
    "        and returns a Spark DataFrame.\n",
    "        args:\n",
    "            table_name: The name of the table to read.\n",
    "            date_value: The date to filter\n",
    "        returns:\n",
    "            A Spark DataFrame containing the filtered data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            def _read():\n",
    "                if (date_value is None):\n",
    "                    query = f\"(SELECT * FROM {table_name}) AS subquery\"\n",
    "                else:\n",
    "                    query = f\"\"\"\n",
    "                        (SELECT *\n",
    "                        FROM {table_name}\n",
    "                        WHERE date = '{date_value}'\n",
    "                        ) AS subquery\n",
    "                    \"\"\"\n",
    "                df = spark.read.jdbc(\n",
    "                    url=self.jdbc_url,\n",
    "                    table=query,  \n",
    "                    properties=self.connection_properties\n",
    "                )\n",
    "                return df\n",
    "            return self.__with_retry(\n",
    "                func=_read,\n",
    "                retries=5,\n",
    "                delay=20,\n",
    "                retry_message=f\"Error reading table {table_name} from database.\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error when reading the table {table_name}: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def execute_jdbc_query(self, query):\n",
    "        \"\"\"\n",
    "        Executes a SQL statement (non-select) using Spark's JVM JDBC connection.\n",
    "        Works in Databricks without needing ODBC drivers.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            jvm = spark._jvm\n",
    "            conn = jvm.java.sql.DriverManager.getConnection(self.jdbc_url, self.username, self.password)\n",
    "            stmt = conn.createStatement()\n",
    "            stmt.execute(query)\n",
    "            stmt.close()\n",
    "            conn.close()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error executing JDBC query: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    def save_table(self, df, container, database_name,storage_account,table_name,date_value):\n",
    "        \"\"\"\n",
    "        Saves a Spark DataFrame to a table in the database.\n",
    "        args:\n",
    "            df: The Spark DataFrame to save.\n",
    "            table_name: The name of the table to create or overwrite.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            client_id=self.client_id\n",
    "            print(f\"client_id: {client_id}\")\n",
    "            spark.conf.set(\n",
    "                \"fs.azure.account.key.smartwalletjorge.dfs.core.windows.net\",\n",
    "                client_id\n",
    "            )\n",
    "            if date_value is None:\n",
    "                root=\"complete\"\n",
    "                container = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{database_name}/{table_name}/{root}\"\n",
    "            else:\n",
    "                root=\"daily\"\n",
    "                container = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{database_name}/{table_name}/{root}/{date_value}\"\n",
    "            df.write.format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .partitionBy(\"date\") \\\n",
    "                .save(container)\n",
    "        except Exception as e:\n",
    "            print(f\"Error when saving the {table_name}: {e}\")  \n",
    "\n",
    "    def read_table_from_path(self, container, database_name, table_name, date_value=None):\n",
    "        \"\"\"\n",
    "        Reads a Delta table from a given path in Azure Data Lake Storage (ADLS).\n",
    "        \n",
    "        Args:\n",
    "            container: The container name in ADLS.\n",
    "            database_name: The name of the logical database/folder.\n",
    "            table_name: The table/folder name.\n",
    "            date_value: The date partition or \"complete\" if not partitioned.\n",
    "\n",
    "        Returns:\n",
    "            A Spark DataFrame with the table content.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            client_id = self.client_id\n",
    "            print(f\"client_id: {client_id}\")\n",
    "            \n",
    "            spark.conf.set(\n",
    "                \"fs.azure.account.key.smartwalletjorge.dfs.core.windows.net\",\n",
    "                client_id\n",
    "            )\n",
    "            \n",
    "            if date_value is None:\n",
    "                path = f\"abfss://{container}@smartwalletjorge.dfs.core.windows.net/{database_name}/{table_name}/*\"\n",
    "            else:\n",
    "                path = f\"abfss://{container}@smartwalletjorge.dfs.core.windows.net/{database_name}/{table_name}/daily/{date_value}\"\n",
    "            \n",
    "            df = spark.read.format(\"delta\").load(path)\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading the table {table_name} from path: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa325ed6-3916-4afb-8748-2fa801e17b5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def safe_sql_value(val):\n",
    "    if val is None or (isinstance(val, float) and np.isnan(val)):\n",
    "        return \"NULL\"\n",
    "    elif isinstance(val, bool):\n",
    "        return \"1\" if val else \"0\"\n",
    "    elif isinstance(val, str):\n",
    "        val = val.replace(\"'\", \"''\")\n",
    "        return f\"'{val}'\"\n",
    "    elif isinstance(val, (pd.Timestamp, np.datetime64, datetime, date)):\n",
    "        return f\"'{str(val)}'\"\n",
    "    else:\n",
    "        return str(val)\n",
    "\n",
    "def safe_clean_cell(x):\n",
    "    try:\n",
    "        if isinstance(x, (dict, list, tuple)):\n",
    "            return str(x)\n",
    "        return x\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error with value: {x} ({type(x)}) → {e}\")\n",
    "        raise\n",
    "\n",
    "def upsert_data(db, table_name, df, pk_columns, logger):\n",
    "    \"\"\"\n",
    "    Insert or update data into a table using JDBC-based SQL execution.\n",
    "\n",
    "    ARGS:\n",
    "        db (Database): db connection object.\n",
    "        table_name (str): table's name\n",
    "        df (DataFrame): pandas dataframe of data to upsert\n",
    "        pk_columns (list): pk columns of the table\n",
    "    RETURNS:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Inserting data in {table_name}\")\n",
    "        print(f\"dataframe sample:\\n{df.head(10)}\")\n",
    "\n",
    "        # Limpieza de datos\n",
    "        df = df.applymap(safe_clean_cell)\n",
    "        df = df.replace({np.nan: None, np.inf: None, -np.inf: None})\n",
    "\n",
    "        # Redondeo de floats\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'float64':\n",
    "                df[col] = df[col].apply(lambda x: round(x, 6) if x is not None else None)\n",
    "\n",
    "        # Construcción del query base con placeholders de formato\n",
    "        columns = df.columns.tolist()\n",
    "        on_clause = \" AND \".join(f\"target.{col} = source.{col}\" for col in pk_columns)\n",
    "        update_clause = \", \".join(f\"target.{col} = source.{col}\" for col in columns if col not in pk_columns)\n",
    "        insert_clause = \", \".join(columns)\n",
    "        values_clause = \", \".join(f\"source.{col}\" for col in columns)\n",
    "        source_clause = \", \".join(\"{{{}}} AS {}\".format(i, col) for i, col in enumerate(columns))  # {0} AS col1, etc.\n",
    "\n",
    "        query_template = f\"\"\"\n",
    "            MERGE INTO {table_name} AS target\n",
    "            USING (SELECT {source_clause}) AS source\n",
    "            ON {on_clause}\n",
    "            WHEN MATCHED THEN \n",
    "                UPDATE SET {update_clause}\n",
    "            WHEN NOT MATCHED THEN \n",
    "                INSERT ({insert_clause})\n",
    "                VALUES ({values_clause});\n",
    "        \"\"\"\n",
    "\n",
    "        # Ejecutar una por una\n",
    "        for row in df.itertuples(index=False, name=None):\n",
    "            formatted_values = [safe_sql_value(v) for v in row]\n",
    "            full_query = query_template.format(*formatted_values)\n",
    "            db.execute_jdbc_query(full_query)\n",
    "\n",
    "        logger.info(f\"✅ Data upserted successfully into {table_name}!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        logger.error(f\"❌ Error upserting data into {table_name}: {e}\")\n",
    "        logger.debug(f\"Último query que falló: {full_query if 'full_query' in locals() else 'N/A'}\")\n",
    "        sys.exit(0)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "database_connector",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
